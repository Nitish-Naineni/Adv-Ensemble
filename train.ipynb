{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "from data import get_data_info\n",
    "from data import load_data\n",
    "\n",
    "from util import format_time\n",
    "from util import Logger\n",
    "from util import Trainer\n",
    "from util import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    augment = 'base'\n",
    "\n",
    "    num_workers = 1\n",
    "    batch_size = 256\n",
    "    batch_size_validation = 256\n",
    "    \n",
    "    data_dir = './data/database/'\n",
    "    log_dir = 'trained_models'\n",
    "        \n",
    "    data = 'cifar10'\n",
    "    desc = 'resnet4_3semble_test_1'\n",
    "\n",
    "    model = \"resnet4\" #'preact-resnet18-swish'\n",
    "    normalize = False\n",
    "    pretrained_file = None\n",
    "\n",
    "    num_adv_epochs = 100 # 400\n",
    "    adv_eval_freq = 25\n",
    "    \n",
    "    beta = 5.0\n",
    "    lamda = 2.0\n",
    "    log_det_lamda = 0.2 #0.5\n",
    "    tau = 0.995\n",
    "    \n",
    "    lr = 0.2\n",
    "    weight_decay = 5e-4\n",
    "    scheduler = 'cosinew'\n",
    "    nesterov = True\n",
    "    clip_grad = None\n",
    "\n",
    "    attack = 'linf-pgd'\n",
    "    attack_eps = 8/255\n",
    "    attack_step = 2/255\n",
    "    attack_iter = 10\n",
    "\n",
    "    debug = False\n",
    "    unsup_fraction = 0.7\n",
    "    seed = 1\n",
    "    resume_path = ''\n",
    "    num_models = 3\n",
    "\n",
    "    #label smoothing\n",
    "    ls = 0.1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to file:  trained_models/resnet4_3semble_test_1/log-train.log\n",
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "Standard Accuracy-\tTest: 10.058594%.\n",
      "RST Adversarial training for 100 epochs\n",
      "======= Epoch 1 =======\n",
      "Loss: 2.3199.\tLR: 0.0080\n",
      "Standard Accuracy-\tTrain: 35.71%.\tTest: 44.46%.\n",
      "Adversarial Accuracy-\tTrain: 29.76%.\n",
      "Adversarial Accuracy-\tEval: 11.43%.\n",
      "Time taken: 0:02:38\n",
      "======= Epoch 2 =======\n",
      "Loss: 2.7236.\tLR: 0.1520\n",
      "Standard Accuracy-\tTrain: 28.87%.\tTest: 33.07%.\n",
      "Adversarial Accuracy-\tTrain: 25.05%.\n",
      "Adversarial Accuracy-\tEval: 10.55%.\n",
      "Time taken: 0:02:38\n",
      "======= Epoch 3 =======\n",
      "Loss: 2.0707.\tLR: 0.2000\n",
      "Standard Accuracy-\tTrain: 43.36%.\tTest: 45.64%.\n",
      "Adversarial Accuracy-\tTrain: 34.41%.\n",
      "Adversarial Accuracy-\tEval: 13.67%.\n",
      "Time taken: 0:02:39\n",
      "======= Epoch 4 =======\n",
      "Loss: 1.8541.\tLR: 0.1999\n",
      "Standard Accuracy-\tTrain: 50.08%.\tTest: 55.86%.\n",
      "Adversarial Accuracy-\tTrain: 37.15%.\n",
      "Adversarial Accuracy-\tEval: 14.55%.\n",
      "Time taken: 0:02:38\n",
      "======= Epoch 5 =======\n",
      "Loss: 1.6867.\tLR: 0.1997\n",
      "Standard Accuracy-\tTrain: 55.29%.\tTest: 60.56%.\n",
      "Adversarial Accuracy-\tTrain: 39.85%.\n",
      "Adversarial Accuracy-\tEval: 17.58%.\n",
      "Time taken: 0:02:39\n",
      "======= Epoch 6 =======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mscheduler:\n\u001b[1;32m     67\u001b[0m     last_lr \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mget_last_lr()[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 69\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(train_dataloader, epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m     70\u001b[0m test_acc \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval(test_dataloader)\n\u001b[1;32m     72\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mLR: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(res[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], last_lr))\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/util/train.py:156\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, dataloader, epoch, verbose)\u001b[0m\n\u001b[1;32m    153\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice,memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mchannels_last), y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    155\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mbeta \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 156\u001b[0m loss, batch_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrades_loss(\n\u001b[1;32m    157\u001b[0m     x, \n\u001b[1;32m    158\u001b[0m     y, \n\u001b[1;32m    159\u001b[0m     beta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mbeta,\n\u001b[1;32m    160\u001b[0m     lamda\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mlamda,\n\u001b[1;32m    161\u001b[0m     log_det_lamda\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mlog_det_lamda\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mclip_grad:\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/util/train.py:189\u001b[0m, in \u001b[0;36mTrainer.trades_loss\u001b[0;34m(self, x, y, beta, lamda, log_det_lamda)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrades_loss\u001b[39m(\u001b[39mself\u001b[39m, x, y, beta, lamda, log_det_lamda):\n\u001b[1;32m    186\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    TRADES training.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     loss, batch_metrics \u001b[39m=\u001b[39m trades_loss(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodels, x, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, step_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack_step, \n\u001b[1;32m    190\u001b[0m                                       epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack_eps, perturb_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack_iter, \n\u001b[1;32m    191\u001b[0m                                       beta\u001b[39m=\u001b[39;49mbeta, attack\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack, label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mls,\n\u001b[1;32m    192\u001b[0m                                       lamda\u001b[39m=\u001b[39;49mlamda,log_det_lamda\u001b[39m=\u001b[39;49mlog_det_lamda,num_classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes)\n\u001b[1;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, batch_metrics\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/util/trades.py:112\u001b[0m, in \u001b[0;36mtrades_loss\u001b[0;34m(models, x_natural, y, optimizer, step_size, epsilon, perturb_steps, attack, beta, lamda, log_det_lamda, label_smoothing, num_classes)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m    111\u001b[0m     loss_kl \u001b[39m=\u001b[39m criterion_kl(F\u001b[39m.\u001b[39mlog_softmax(ensemble(x_adv), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), p_natural)\n\u001b[0;32m--> 112\u001b[0m grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(loss_kl, [x_adv])[\u001b[39m0\u001b[39m]\n\u001b[1;32m    113\u001b[0m x_adv \u001b[39m=\u001b[39m x_adv\u001b[39m.\u001b[39mdetach() \u001b[39m+\u001b[39m step_size \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msign(grad\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m    114\u001b[0m x_adv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(torch\u001b[39m.\u001b[39mmax(x_adv, x_natural \u001b[39m-\u001b[39m epsilon), x_natural \u001b[39m+\u001b[39m epsilon)\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(args.data_dir, args.data)\n",
    "LOG_DIR = os.path.join(args.log_dir, args.desc)\n",
    "WEIGHTS = os.path.join(LOG_DIR, 'weights-best.pt')\n",
    "if os.path.exists(LOG_DIR):\n",
    "    shutil.rmtree(LOG_DIR)\n",
    "os.makedirs(LOG_DIR)\n",
    "logger = Logger(os.path.join(LOG_DIR, 'log-train.log'))\n",
    "\n",
    "with open(os.path.join(LOG_DIR, 'args.txt'), 'w') as f:\n",
    "    json.dump(args.__dict__, f, indent=4)\n",
    "\n",
    "\n",
    "info = get_data_info(DATA_DIR)\n",
    "BATCH_SIZE = args.batch_size\n",
    "BATCH_SIZE_VALIDATION = args.batch_size_validation\n",
    "NUM_ADV_EPOCHS = args.num_adv_epochs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.log('Using device: {}'.format(device))\n",
    "if args.debug:\n",
    "    NUM_ADV_EPOCHS = 1\n",
    "\n",
    "# To speed up training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "seed(args.seed)\n",
    "train_dataloader, test_dataloader, eval_dataloader = load_data(\n",
    "    data_dir=DATA_DIR, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batch_size_test=BATCH_SIZE_VALIDATION, \n",
    "    augmentation=args.augment, \n",
    "    unsup_fraction=args.unsup_fraction,\n",
    "    num_workers=args.num_workers\n",
    ")\n",
    "\n",
    "# Adversarial Training\n",
    "\n",
    "seed(args.seed)\n",
    "trainer = Trainer(info, args)\n",
    "\n",
    "last_lr = args.lr\n",
    "\n",
    "\n",
    "if NUM_ADV_EPOCHS > 0:\n",
    "    logger.log('\\n\\n')\n",
    "    metrics = pd.DataFrame()\n",
    "    logger.log('Standard Accuracy-\\tTest: {:2f}%.'.format(trainer.eval(test_dataloader)*100))\n",
    "    \n",
    "    old_score = [0.0, 0.0]\n",
    "    logger.log('RST Adversarial training for {} epochs'.format(NUM_ADV_EPOCHS))\n",
    "    trainer.init_optimizer(args.num_adv_epochs)\n",
    "    test_adv_acc = 0.0    \n",
    "\n",
    "if args.resume_path:\n",
    "    start_epoch = trainer.load_model_resume(os.path.join(args.resume_path, 'state-last.pt')) + 1\n",
    "    logger.log(f'Resuming at epoch {start_epoch}')\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "for epoch in range(start_epoch, NUM_ADV_EPOCHS+1):\n",
    "    start = time.time()\n",
    "    logger.log('======= Epoch {} ======='.format(epoch))\n",
    "    \n",
    "    if args.scheduler:\n",
    "        last_lr = trainer.scheduler.get_last_lr()[0]\n",
    "    \n",
    "    res = trainer.train(train_dataloader, epoch=epoch)\n",
    "    test_acc = trainer.eval(test_dataloader)\n",
    "    \n",
    "    logger.log('Loss: {:.4f}.\\tLR: {:.4f}'.format(res['loss'], last_lr))\n",
    "    if 'clean_acc' in res:\n",
    "        logger.log('Standard Accuracy-\\tTrain: {:.2f}%.\\tTest: {:.2f}%.'.format(res['clean_acc']*100, test_acc*100))\n",
    "    else:\n",
    "        logger.log('Standard Accuracy-\\tTest: {:.2f}%.'.format(test_acc*100))\n",
    "    epoch_metrics = {'train_'+k: v for k, v in res.items()}\n",
    "    epoch_metrics.update({'epoch': epoch, 'lr': last_lr, 'test_clean_acc': test_acc, 'test_adversarial_acc': ''})\n",
    "\n",
    "    if epoch % args.adv_eval_freq == 0 or epoch == NUM_ADV_EPOCHS:        \n",
    "        test_adv_acc = trainer.eval(test_dataloader, adversarial=True)\n",
    "        logger.log('Adversarial Accuracy-\\tTrain: {:.2f}%.\\tTest: {:.2f}%.'.format(res['adversarial_acc']*100, \n",
    "                                                                                test_adv_acc*100))\n",
    "        epoch_metrics.update({'test_adversarial_acc': test_adv_acc})\n",
    "    else:\n",
    "        logger.log('Adversarial Accuracy-\\tTrain: {:.2f}%.'.format(res['adversarial_acc']*100))\n",
    "    eval_adv_acc = trainer.eval(eval_dataloader, adversarial=True)\n",
    "    logger.log('Adversarial Accuracy-\\tEval: {:.2f}%.'.format(eval_adv_acc*100))\n",
    "    epoch_metrics['eval_adversarial_acc'] = eval_adv_acc\n",
    "\n",
    "    \n",
    "    if eval_adv_acc >= old_score[1]:\n",
    "        old_score[0], old_score[1] = test_acc, eval_adv_acc\n",
    "        trainer.save_model(WEIGHTS)\n",
    "    # trainer.save_model(os.path.join(LOG_DIR, 'weights-last.pt'))\n",
    "    if epoch % 10 == 0:\n",
    "        trainer.save_model_resume(os.path.join(LOG_DIR, 'state-last.pt'), epoch) \n",
    "    if epoch % 400 == 0:\n",
    "        shutil.copyfile(WEIGHTS, os.path.join(LOG_DIR, f'weights-best-epoch{str(epoch)}.pt'))\n",
    "\n",
    "    logger.log('Time taken: {}'.format(format_time(time.time()-start)))\n",
    "    metrics = metrics.append(pd.DataFrame(epoch_metrics, index=[0]), ignore_index=True)\n",
    "    metrics.to_csv(os.path.join(LOG_DIR, 'stats_adv.csv'), index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "# Record metrics\n",
    "\n",
    "train_acc = res['clean_acc'] if 'clean_acc' in res else trainer.eval(train_dataloader)\n",
    "logger.log('\\nTraining completed.')\n",
    "logger.log('Standard Accuracy-\\tTrain: {:.2f}%.\\tTest: {:.2f}%.'.format(train_acc*100, old_score[0]*100))\n",
    "if NUM_ADV_EPOCHS > 0 and arg.adversarial:\n",
    "    logger.log('Adversarial Accuracy-\\tTrain: {:.2f}%.\\tEval: {:.2f}%.'.format(res['adversarial_acc']*100, old_score[1]*100)) \n",
    "\n",
    "logger.log('Script Completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
