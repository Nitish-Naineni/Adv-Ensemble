{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "from data import get_data_info\n",
    "from data import load_data\n",
    "\n",
    "from util import format_time\n",
    "from util import Logger\n",
    "from util import Trainer\n",
    "from util import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    augment = 'base'\n",
    "\n",
    "    num_workers = 4\n",
    "    batch_size = 1024\n",
    "    batch_size_validation = 512\n",
    "    \n",
    "    data_dir = '/cluster/home/rarade/data/'\n",
    "    log_dir = '/cluster/scratch/rarade/test/'\n",
    "        \n",
    "    data = 'cifar10s'\n",
    "    desc = None\n",
    "\n",
    "    model = 'wrn-28-10-swish'\n",
    "    normalize = False\n",
    "    pretrained_file = None\n",
    "\n",
    "    num_adv_epochs = 100\n",
    "    adv_eval_freq = 25\n",
    "    \n",
    "    beta = None\n",
    "    lamda = None\n",
    "    log_det_lamda = None\n",
    "    \n",
    "    lr = 0.4\n",
    "    weight_decay = 5e-4\n",
    "    scheduler = 'cosinew'\n",
    "    nesterov = True\n",
    "    clip_grad = None\n",
    "\n",
    "    attack = 'linf-pgd'\n",
    "    attack_eps = 8/255\n",
    "    attack_step = 2/255\n",
    "    attack_iter = 10\n",
    "    keep_clean = False\n",
    "\n",
    "    debug = False\n",
    "    mart = False\n",
    "    \n",
    "    unsup_fraction = 0.7\n",
    "    aux_data_filename = '/cluster/scratch/rarade/cifar10s/ti_500K_pseudo_labeled.pickle'\n",
    "    \n",
    "    seed = 1\n",
    "\n",
    "    ### Consistency\n",
    "    consistency = False\n",
    "    cons_lambda = 1.0\n",
    "    cons_tem = 0.5\n",
    "\n",
    "    ### Resume\n",
    "    resume_path = ''\n",
    "\n",
    "    ### Our methods\n",
    "    LSE = False\n",
    "    ls = 0.1\n",
    "    clip_value = 0\n",
    "    CutMix = False\n",
    "\n",
    "    ### Ensemble\n",
    "    num_models = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "args.num_workers=1\n",
    "args.data_dir = '/home/nitish/projects/Adv-Ensemble/data/database/'\n",
    "args.log_dir = 'trained_models'\n",
    "args.desc = 'preact-resnet18-swish_3semble'\n",
    "args.data = 'cifar10'\n",
    "args.num_workers = 1\n",
    "args.batch_size = 256 #512\n",
    "args.batch_size_validation = 256\n",
    "args.model = 'preact-resnet18-swish'\n",
    "args.num_adv_epochs = 50 #400\n",
    "args.lr = 0.2\n",
    "args.beta = 5.0\n",
    "args.lamda = 2.0\n",
    "args.log_det_lamda = 0.5\n",
    "args.unsup_fraction = 0.7\n",
    "args.ls = 0.1\n",
    "args.tau = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to file:  trained_models/preact-resnet18-swish_3semble/log-train.log\n",
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "Standard Accuracy-\tTest: 10.761719%.\n",
      "RST Adversarial training for 50 epochs\n",
      "======= Epoch 1 =======\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity; 11.76 GiB already allocated; 45.25 MiB free; 11.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mscheduler:\n\u001b[1;32m     67\u001b[0m     last_lr \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mget_last_lr()[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 69\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(train_dataloader, epoch\u001b[39m=\u001b[39;49mepoch, adversarial\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     70\u001b[0m test_acc \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval(test_dataloader)\n\u001b[1;32m     72\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mLR: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(res[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], last_lr))\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/util/train.py:186\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, dataloader, epoch, adversarial, verbose)\u001b[0m\n\u001b[1;32m    184\u001b[0m     loss, batch_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrades_loss_LSE(x, y, beta\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mbeta)\n\u001b[1;32m    185\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mbeta \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     loss, batch_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrades_loss(\n\u001b[1;32m    187\u001b[0m         x, \n\u001b[1;32m    188\u001b[0m         y, \n\u001b[1;32m    189\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mbeta,\n\u001b[1;32m    190\u001b[0m         lamda\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mlamda,\n\u001b[1;32m    191\u001b[0m         log_det_lamda\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mlog_det_lamda\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     loss, batch_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madversarial_loss(x, y)\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/util/train.py:231\u001b[0m, in \u001b[0;36mTrainer.trades_loss\u001b[0;34m(self, x, y, beta, lamda, log_det_lamda)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrades_loss\u001b[39m(\u001b[39mself\u001b[39m, x, y, beta, lamda, log_det_lamda):\n\u001b[1;32m    228\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39m    TRADES training.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     loss, batch_metrics \u001b[39m=\u001b[39m trades_loss(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodels, x, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, step_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack_step, \n\u001b[1;32m    232\u001b[0m                                       epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack_eps, perturb_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack_iter, \n\u001b[1;32m    233\u001b[0m                                       beta\u001b[39m=\u001b[39;49mbeta, attack\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mattack, label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mls,\n\u001b[1;32m    234\u001b[0m                                       lamda\u001b[39m=\u001b[39;49mlamda,log_det_lamda\u001b[39m=\u001b[39;49mlog_det_lamda,num_classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes)\n\u001b[1;32m    235\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, batch_metrics\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/util/trades.py:127\u001b[0m, in \u001b[0;36mtrades_loss\u001b[0;34m(models, x_natural, y, optimizer, step_size, epsilon, perturb_steps, attack, beta, lamda, log_det_lamda, label_smoothing, num_classes)\u001b[0m\n\u001b[1;32m    124\u001b[0m loss_std \u001b[39m=\u001b[39m adp_loss(x_natural, y, models, criterion_ce, lamda, log_det_lamda, num_classes)\n\u001b[1;32m    126\u001b[0m logits_natural \u001b[39m=\u001b[39m ensemble(x_natural)\n\u001b[0;32m--> 127\u001b[0m logits_adv \u001b[39m=\u001b[39m ensemble(x_adv)\n\u001b[1;32m    129\u001b[0m loss_robust \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion_kl(F\u001b[39m.\u001b[39mlog_softmax(logits_adv, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), F\u001b[39m.\u001b[39msoftmax(logits_natural, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    131\u001b[0m loss \u001b[39m=\u001b[39m loss_std \u001b[39m+\u001b[39m beta \u001b[39m*\u001b[39m loss_robust\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/util/trades.py:21\u001b[0m, in \u001b[0;36mEnsemble.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels:\n\u001b[0;32m---> 21\u001b[0m     outputs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(model(x), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m output \u001b[39m=\u001b[39m outputs \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels)\n\u001b[1;32m     23\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(output, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1e-40\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     kwargs \u001b[39m=\u001b[39m ({},)\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/models/preact_resnetwithswish.py:137\u001b[0m, in \u001b[0;36mPreActResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m     out \u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstd\n\u001b[1;32m    136\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_2d(out)\n\u001b[0;32m--> 137\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_0(out)\n\u001b[1;32m    138\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_1(out)\n\u001b[1;32m    139\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_2(out)\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/Adv-Ensemble/models/preact_resnetwithswish.py:64\u001b[0m, in \u001b[0;36m_PreActBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m shortcut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pad(x)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_shortcut \u001b[39melse\u001b[39;00m x\n\u001b[1;32m     63\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_2d_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pad(out))\n\u001b[0;32m---> 64\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_2d_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatchnorm_1(out)))\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m out \u001b[39m+\u001b[39m shortcut\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/modules/activation.py:396\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/mambaforge/envs/bae/lib/python3.10/site-packages/torch/nn/functional.py:2059\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   2058\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39msilu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 2059\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity; 11.76 GiB already allocated; 45.25 MiB free; 11.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(args.data_dir, args.data)\n",
    "LOG_DIR = os.path.join(args.log_dir, args.desc)\n",
    "WEIGHTS = os.path.join(LOG_DIR, 'weights-best.pt')\n",
    "if os.path.exists(LOG_DIR):\n",
    "    shutil.rmtree(LOG_DIR)\n",
    "os.makedirs(LOG_DIR)\n",
    "logger = Logger(os.path.join(LOG_DIR, 'log-train.log'))\n",
    "\n",
    "with open(os.path.join(LOG_DIR, 'args.txt'), 'w') as f:\n",
    "    json.dump(args.__dict__, f, indent=4)\n",
    "\n",
    "\n",
    "info = get_data_info(DATA_DIR)\n",
    "BATCH_SIZE = args.batch_size\n",
    "BATCH_SIZE_VALIDATION = args.batch_size_validation\n",
    "NUM_ADV_EPOCHS = args.num_adv_epochs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.log('Using device: {}'.format(device))\n",
    "if args.debug:\n",
    "    NUM_ADV_EPOCHS = 1\n",
    "\n",
    "# To speed up training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "seed(args.seed)\n",
    "train_dataloader, test_dataloader, eval_dataloader = load_data(\n",
    "    data_dir=DATA_DIR, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batch_size_test=BATCH_SIZE_VALIDATION, \n",
    "    augmentation=args.augment, \n",
    "    unsup_fraction=args.unsup_fraction,\n",
    "    num_workers=args.num_workers\n",
    ")\n",
    "\n",
    "# Adversarial Training\n",
    "\n",
    "seed(args.seed)\n",
    "trainer = Trainer(info, args)\n",
    "\n",
    "last_lr = args.lr\n",
    "\n",
    "\n",
    "if NUM_ADV_EPOCHS > 0:\n",
    "    logger.log('\\n\\n')\n",
    "    metrics = pd.DataFrame()\n",
    "    logger.log('Standard Accuracy-\\tTest: {:2f}%.'.format(trainer.eval(test_dataloader)*100))\n",
    "    \n",
    "    old_score = [0.0, 0.0]\n",
    "    logger.log('RST Adversarial training for {} epochs'.format(NUM_ADV_EPOCHS))\n",
    "    trainer.init_optimizer(args.num_adv_epochs)\n",
    "    test_adv_acc = 0.0    \n",
    "\n",
    "if args.resume_path:\n",
    "    start_epoch = trainer.load_model_resume(os.path.join(args.resume_path, 'state-last.pt')) + 1\n",
    "    logger.log(f'Resuming at epoch {start_epoch}')\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "for epoch in range(start_epoch, NUM_ADV_EPOCHS+1):\n",
    "    start = time.time()\n",
    "    logger.log('======= Epoch {} ======='.format(epoch))\n",
    "    \n",
    "    if args.scheduler:\n",
    "        last_lr = trainer.scheduler.get_last_lr()[0]\n",
    "    \n",
    "    res = trainer.train(train_dataloader, epoch=epoch, adversarial=True)\n",
    "    test_acc = trainer.eval(test_dataloader)\n",
    "    \n",
    "    logger.log('Loss: {:.4f}.\\tLR: {:.4f}'.format(res['loss'], last_lr))\n",
    "    if 'clean_acc' in res:\n",
    "        logger.log('Standard Accuracy-\\tTrain: {:.2f}%.\\tTest: {:.2f}%.'.format(res['clean_acc']*100, test_acc*100))\n",
    "    else:\n",
    "        logger.log('Standard Accuracy-\\tTest: {:.2f}%.'.format(test_acc*100))\n",
    "    epoch_metrics = {'train_'+k: v for k, v in res.items()}\n",
    "    epoch_metrics.update({'epoch': epoch, 'lr': last_lr, 'test_clean_acc': test_acc, 'test_adversarial_acc': ''})\n",
    "    \n",
    "    if epoch % args.adv_eval_freq == 0 or epoch == NUM_ADV_EPOCHS:        \n",
    "        test_adv_acc = trainer.eval(test_dataloader, adversarial=True)\n",
    "        logger.log('Adversarial Accuracy-\\tTrain: {:.2f}%.\\tTest: {:.2f}%.'.format(res['adversarial_acc']*100, \n",
    "                                                                                   test_adv_acc*100))\n",
    "        epoch_metrics.update({'test_adversarial_acc': test_adv_acc})\n",
    "    else:\n",
    "        logger.log('Adversarial Accuracy-\\tTrain: {:.2f}%.'.format(res['adversarial_acc']*100))\n",
    "    eval_adv_acc = trainer.eval(eval_dataloader, adversarial=True)\n",
    "    logger.log('Adversarial Accuracy-\\tEval: {:.2f}%.'.format(eval_adv_acc*100))\n",
    "    epoch_metrics['eval_adversarial_acc'] = eval_adv_acc\n",
    "    \n",
    "    if eval_adv_acc >= old_score[1]:\n",
    "        old_score[0], old_score[1] = test_acc, eval_adv_acc\n",
    "        trainer.save_model(WEIGHTS)\n",
    "    # trainer.save_model(os.path.join(LOG_DIR, 'weights-last.pt'))\n",
    "    if epoch % 10 == 0:\n",
    "        trainer.save_model_resume(os.path.join(LOG_DIR, 'state-last.pt'), epoch) \n",
    "    if epoch % 400 == 0:\n",
    "        shutil.copyfile(WEIGHTS, os.path.join(LOG_DIR, f'weights-best-epoch{str(epoch)}.pt'))\n",
    "\n",
    "    logger.log('Time taken: {}'.format(format_time(time.time()-start)))\n",
    "    metrics = metrics.append(pd.DataFrame(epoch_metrics, index=[0]), ignore_index=True)\n",
    "    metrics.to_csv(os.path.join(LOG_DIR, 'stats_adv.csv'), index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "# Record metrics\n",
    "\n",
    "train_acc = res['clean_acc'] if 'clean_acc' in res else trainer.eval(train_dataloader)\n",
    "logger.log('\\nTraining completed.')\n",
    "logger.log('Standard Accuracy-\\tTrain: {:.2f}%.\\tTest: {:.2f}%.'.format(train_acc*100, old_score[0]*100))\n",
    "if NUM_ADV_EPOCHS > 0:\n",
    "    logger.log('Adversarial Accuracy-\\tTrain: {:.2f}%.\\tEval: {:.2f}%.'.format(res['adversarial_acc']*100, old_score[1]*100)) \n",
    "\n",
    "logger.log('Script Completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
